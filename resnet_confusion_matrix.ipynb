{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix for ResNet Model\n",
    "\n",
    "This notebook creates a confusion matrix to evaluate the ResNet classification model performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Device and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using MPS (Apple Silicon)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/resnet/model_info.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m model_dir = Path(\u001b[33m'\u001b[39m\u001b[33mmodels/resnet\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load model info\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_info.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     22\u001b[39m     model_info = json.load(f)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Load label mappings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/doodle-recognition/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'models/resnet/model_info.json'"
     ]
    }
   ],
   "source": [
    "# Setup device (GPU/MPS/CPU)\n",
    "def setup_device():\n",
    "    \"\"\"Setup device for inference (CUDA/MPS/CPU)\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"✓ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(\"✓ Using MPS (Apple Silicon)\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"⚠ Using CPU\")\n",
    "    return device\n",
    "\n",
    "device = setup_device()\n",
    "\n",
    "# Load model info and label mappings\n",
    "model_dir = Path('./models/resnet')\n",
    "\n",
    "# Load model info\n",
    "with open(model_dir / 'model_info.json', 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "# Load label mappings\n",
    "with open(model_dir / 'label_mappings.json', 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "# Extract information\n",
    "idx_to_label = {int(k): v for k, v in label_mappings['idx_to_label'].items()}\n",
    "label_to_idx = label_mappings['label_to_idx']\n",
    "num_classes = model_info['num_classes']\n",
    "img_size = model_info['img_size']\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Image size: {img_size}x{img_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Test accuracy (from training): {model_info.get('test_accuracy', 'N/A')}\")\n",
    "print(f\"  Test top-3 accuracy (from training): {model_info.get('test_top3_accuracy', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Load ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create a ResNet18 model matching the training architecture\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=None)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_resnet_model(num_classes)\n",
    "\n",
    "# Load trained weights\n",
    "model_path = model_dir / 'model.pth'\n",
    "if not model_path.exists():\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded successfully from {model_path}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Define transforms (matching training - ImageNet normalization)\n",
    "# CRITICAL FIX: Training uses ImageNet normalization [0.485, 0.456, 0.406] / [0.229, 0.224, 0.225]\n",
    "# NOT [0.5, 0.5, 0.5] / [0.5, 0.5, 0.5]. This mismatch causes terrible accuracy!\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "doodle_dir = Path('doodles/doodle')\n",
    "\n",
    "# Configuration\n",
    "SAMPLES_PER_CATEGORY = 200  # Number of test samples per category\n",
    "\n",
    "def load_test_images(categories, samples_per_category, image_size):\n",
    "    \"\"\"\n",
    "    Load test images from disk and preprocess them for ResNet.\n",
    "    ResNet expects RGB images (3 channels).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for category in tqdm(categories, desc=\"Loading test images\"):\n",
    "        if category not in label_to_idx:\n",
    "            continue\n",
    "            \n",
    "        category_path = doodle_dir / category\n",
    "        if not category_path.exists():\n",
    "            continue\n",
    "            \n",
    "        image_files = list(category_path.glob('*.png'))[:samples_per_category]\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            try:\n",
    "                # Load image as RGB (matching training - no inversion)\n",
    "                # The training dataset doesn't invert images, just loads RGB directly\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                \n",
    "                images.append(img)\n",
    "                labels.append(category)\n",
    "                image_paths.append(str(img_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return images, np.array(labels), image_paths\n",
    "\n",
    "# Get all categories that are in the model\n",
    "all_categories = sorted([d.name for d in doodle_dir.iterdir() if d.is_dir()])\n",
    "model_categories = [cat for cat in all_categories if cat in label_to_idx]\n",
    "\n",
    "print(f\"Loading test data from {len(model_categories)} categories...\")\n",
    "print(f\"Samples per category: {SAMPLES_PER_CATEGORY}\")\n",
    "\n",
    "# Load test images\n",
    "X_test_images, y_test_labels, test_paths = load_test_images(\n",
    "    model_categories, SAMPLES_PER_CATEGORY, img_size\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "y_test_encoded = np.array([label_to_idx[label] for label in y_test_labels])\n",
    "\n",
    "print(f\"\\nLoaded {len(X_test_images)} test images\")\n",
    "print(f\"Number of classes in test set: {len(np.unique(y_test_encoded))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred_encoded = []\n",
    "\n",
    "# Process in batches for efficiency\n",
    "BATCH_SIZE = 64\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(X_test_images), BATCH_SIZE), desc=\"Predicting\"):\n",
    "        batch_images = X_test_images[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Transform images\n",
    "        batch_tensors = torch.stack([transform(img) for img in batch_images]).to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(batch_tensors)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_pred_encoded.extend(predicted.cpu().numpy())\n",
    "\n",
    "y_pred_encoded = np.array(y_pred_encoded)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Convert back to labels for better readability\n",
    "y_test_label_names = [idx_to_label[idx] for idx in y_test_encoded]\n",
    "y_pred_label_names = [idx_to_label[idx] for idx in y_pred_encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_encoded, y_pred_encoded, labels=list(range(num_classes)))\n",
    "\n",
    "print(f\"Confusion matrix shape: {cm.shape}\")\n",
    "print(f\"Total test samples: {len(y_test_encoded)}\")\n",
    "print(f\"Correct predictions: {np.trace(cm)}\")\n",
    "print(f\"Accuracy from confusion matrix: {np.trace(cm) / len(y_test_encoded):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Confusion Matrix (Full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 340 classes, the full confusion matrix is too large to visualize clearly\n",
    "# We'll create a normalized version and show statistics\n",
    "\n",
    "# Normalize confusion matrix (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_normalized = np.nan_to_num(cm_normalized)  # Replace NaN with 0\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot raw confusion matrix (log scale for better visibility)\n",
    "cm_log = np.log1p(cm)  # log(1+x) to handle zeros\n",
    "im1 = axes[0].imshow(cm_log, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title(f'Confusion Matrix (Log Scale)\\nAccuracy: {accuracy:.4f}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "im2 = axes[1].imshow(cm_normalized, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('Normalized Confusion Matrix\\n(Row-wise percentages)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Due to the large number of classes (340), individual labels are not shown.\")\n",
    "print(\"The diagonal represents correct predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Top Confused Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find classes with most confusion (off-diagonal elements)\n",
    "off_diagonal = cm.copy()\n",
    "np.fill_diagonal(off_diagonal, 0)\n",
    "\n",
    "# Get top N most confused classes\n",
    "top_n = 20\n",
    "class_totals = cm.sum(axis=1)\n",
    "class_correct = np.diag(cm)\n",
    "class_errors = class_totals - class_correct\n",
    "\n",
    "# Get indices sorted by error count\n",
    "top_error_indices = np.argsort(class_errors)[-top_n:][::-1]\n",
    "\n",
    "# Create submatrix for top confused classes\n",
    "cm_subset = cm[np.ix_(top_error_indices, top_error_indices)]\n",
    "labels_subset = [idx_to_label[idx] for idx in top_error_indices]\n",
    "\n",
    "# Plot subset confusion matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "cm_subset_normalized = cm_subset.astype('float') / cm_subset.sum(axis=1)[:, np.newaxis]\n",
    "cm_subset_normalized = np.nan_to_num(cm_subset_normalized)\n",
    "\n",
    "sns.heatmap(cm_subset_normalized, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='Blues',\n",
    "            xticklabels=labels_subset,\n",
    "            yticklabels=labels_subset,\n",
    "            cbar_kws={'label': 'Normalized Count'})\n",
    "\n",
    "plt.title(f'Confusion Matrix - Top {top_n} Most Confused Classes\\n(Normalized by row)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics for top confused classes\n",
    "print(f\"\\nTop {top_n} classes with most errors:\")\n",
    "print(\"=\" * 80)\n",
    "for idx in top_error_indices:\n",
    "    label = idx_to_label[idx]\n",
    "    total = class_totals[idx]\n",
    "    correct = class_correct[idx]\n",
    "    errors = class_errors[idx]\n",
    "    accuracy_class = correct / total if total > 0 else 0\n",
    "    print(f\"{label:30s} | Total: {total:4d} | Correct: {correct:4d} | Errors: {errors:4d} | Acc: {accuracy_class:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Class Accuracy Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "class_accuracies = []\n",
    "for idx in range(num_classes):\n",
    "    total = cm.sum(axis=1)[idx]\n",
    "    if total > 0:\n",
    "        correct = cm[idx, idx]\n",
    "        accuracy_class = correct / total\n",
    "        class_accuracies.append({\n",
    "            'class_idx': idx,\n",
    "            'label': idx_to_label[idx],\n",
    "            'total_samples': total,\n",
    "            'correct': correct,\n",
    "            'accuracy': accuracy_class\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_acc = pd.DataFrame(class_accuracies)\n",
    "df_acc = df_acc.sort_values('accuracy')\n",
    "\n",
    "# Display statistics\n",
    "print(\"Per-Class Accuracy Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mean accuracy: {df_acc['accuracy'].mean():.4f}\")\n",
    "print(f\"Median accuracy: {df_acc['accuracy'].median():.4f}\")\n",
    "print(f\"Std deviation: {df_acc['accuracy'].std():.4f}\")\n",
    "print(f\"\\nBest performing classes:\")\n",
    "print(df_acc.tail(10)[['label', 'total_samples', 'correct', 'accuracy']].to_string(index=False))\n",
    "print(f\"\\nWorst performing classes:\")\n",
    "print(df_acc.head(10)[['label', 'total_samples', 'correct', 'accuracy']].to_string(index=False))\n",
    "\n",
    "# Plot distribution of accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_acc['accuracy'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Per-Class Accuracy', fontsize=12)\n",
    "plt.ylabel('Number of Classes', fontsize=12)\n",
    "plt.title('Distribution of Per-Class Accuracies', fontsize=14, fontweight='bold')\n",
    "plt.axvline(df_acc['accuracy'].mean(), color='red', linestyle='--', label=f'Mean: {df_acc[\"accuracy\"].mean():.3f}')\n",
    "plt.axvline(df_acc['accuracy'].median(), color='green', linestyle='--', label=f'Median: {df_acc[\"accuracy\"].median():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "class_names = [idx_to_label[i] for i in range(num_classes)]\n",
    "report = classification_report(y_test_encoded, y_pred_encoded, \n",
    "                               target_names=class_names,\n",
    "                               output_dict=True,\n",
    "                               zero_division=0)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"\\nMacro Average:\")\n",
    "print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
    "print(f\"  Recall: {report['macro avg']['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {report['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"\\nWeighted Average:\")\n",
    "print(f\"  Precision: {report['weighted avg']['precision']:.4f}\")\n",
    "print(f\"  Recall: {report['weighted avg']['recall']:.4f}\")\n",
    "print(f\"  F1-Score: {report['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Top-3 Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate top-3 accuracy\n",
    "print(\"Calculating top-3 accuracy...\")\n",
    "top3_correct = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(X_test_images), BATCH_SIZE), desc=\"Top-3 predictions\"):\n",
    "        batch_images = X_test_images[i:i+BATCH_SIZE]\n",
    "        batch_labels = y_test_encoded[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Transform images\n",
    "        batch_tensors = torch.stack([transform(img) for img in batch_images]).to(device)\n",
    "        \n",
    "        # Get top-3 predictions\n",
    "        outputs = model(batch_tensors)\n",
    "        _, top3_pred = torch.topk(outputs, 3, dim=1)\n",
    "        \n",
    "        # Check if true label is in top-3\n",
    "        for j, true_label in enumerate(batch_labels):\n",
    "            if true_label in top3_pred[j].cpu().numpy():\n",
    "                top3_correct += 1\n",
    "\n",
    "top3_accuracy = top3_correct / len(y_test_encoded)\n",
    "print(f\"\\nTop-3 Accuracy: {top3_accuracy:.4f} ({top3_accuracy*100:.2f}%)\")\n",
    "print(f\"Top-1 Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Improvement: {(top3_accuracy - accuracy):.4f} ({(top3_accuracy - accuracy)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix and results\n",
    "output_dir = Path('results/resnet')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save confusion matrix as numpy array\n",
    "np.save(output_dir / 'confusion_matrix.npy', cm)\n",
    "np.save(output_dir / 'confusion_matrix_normalized.npy', cm_normalized)\n",
    "\n",
    "# Save per-class accuracies\n",
    "df_acc.to_csv(output_dir / 'per_class_accuracy.csv', index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary = {\n",
    "    'test_accuracy': float(accuracy),\n",
    "    'test_top3_accuracy': float(top3_accuracy),\n",
    "    'num_test_samples': int(len(y_test_encoded)),\n",
    "    'num_classes': int(num_classes),\n",
    "    'mean_per_class_accuracy': float(df_acc['accuracy'].mean()),\n",
    "    'median_per_class_accuracy': float(df_acc['accuracy'].median()),\n",
    "    'std_per_class_accuracy': float(df_acc['accuracy'].std()),\n",
    "    'macro_avg_precision': float(report['macro avg']['precision']),\n",
    "    'macro_avg_recall': float(report['macro avg']['recall']),\n",
    "    'macro_avg_f1': float(report['macro avg']['f1-score']),\n",
    "    'weighted_avg_precision': float(report['weighted avg']['precision']),\n",
    "    'weighted_avg_recall': float(report['weighted avg']['recall']),\n",
    "    'weighted_avg_f1': float(report['weighted avg']['f1-score'])\n",
    "}\n",
    "\n",
    "with open(output_dir / 'evaluation_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {output_dir}/\")\n",
    "print(f\"  - confusion_matrix.npy\")\n",
    "print(f\"  - confusion_matrix_normalized.npy\")\n",
    "print(f\"  - per_class_accuracy.csv\")\n",
    "print(f\"  - evaluation_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
